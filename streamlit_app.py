
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.cluster import KMeans
from mlxtend.frequent_patterns import apriori, association_rules

import plotly.express as px
import plotly.graph_objects as go
import ml_utils as mu
import io

st.set_page_config(page_title="GCC Warehousing Insights Dashboard", layout="wide", page_icon="üì¶")
st.title("üì¶ AI‚ÄëDriven Warehousing & Fulfillment ‚Äì Insights Dashboard")

# ------------------------------------------------------------------
# Data loading
# ------------------------------------------------------------------
@st.cache_data
def load_data(path_or_url: str = "synthetic_gcc_warehousing_survey.csv") -> pd.DataFrame:
    try:
        if path_or_url.startswith("http"):
            return pd.read_csv(path_or_url)
        return pd.read_csv(path_or_url)
    except Exception as e:
        st.error(f"Failed to load default data: {e}")
        return pd.DataFrame()

default_df = load_data()

uploaded = st.sidebar.file_uploader("‚¨ÜÔ∏è Upload your survey CSV (optional)", type=["csv"])
if uploaded:
    df = pd.read_csv(uploaded)
else:
    df = default_df.copy()

if df.empty:
    st.warning("No data available ‚Äî please upload a CSV.")
    st.stop()

# ------------------------------------------------------------------
# Sidebar Filters
# ------------------------------------------------------------------
st.sidebar.header("Filter Data")
country_filter = st.sidebar.multiselect("Country", df["Country"].unique(), default=list(df["Country"].unique()))
company_filter = st.sidebar.multiselect("Company Type", df["CompanyType"].unique(), default=list(df["CompanyType"].unique()))

filtered_df = df[(df["Country"].isin(country_filter)) & (df["CompanyType"].isin(company_filter))]

tab1, tab2, tab3, tab4, tab5 = st.tabs(["üîç Data Visualization", "ü§ñ Classification", "üìä Clustering", "üõí Association Rules", "üìà Regression"])

# ------------------------------------------------------------------
# Tab 1: Data Visualization
# ------------------------------------------------------------------
with tab1:
    st.subheader("Descriptive Insights")
    st.markdown("Below are interactive charts unveiling key patterns in the survey.")

    col1, col2 = st.columns(2)
    with col1:
        fig = px.histogram(filtered_df, x="AnnualRevenue", color="Country", barmode="group", title="Revenue Distribution by Country")
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        fig = px.box(filtered_df, x="CompanyType", y="SustainabilityImportance", color="CompanyType",
                     title="Sustainability Importance by Company Type")
        st.plotly_chart(fig, use_container_width=True)

    # Additional insights
    st.markdown("### Likelihood to Use Platform vs. Pain Points")
    tmp = filtered_df.explode("PainPointsCurrentModel")
    fig = px.box(tmp, x="PainPointsCurrentModel", y="LikelihoodToUseAIPlatform", title="Pain Points vs. Adoption Likelihood")
    st.plotly_chart(fig, use_container_width=True)

    st.markdown("### Top Main Logistics Challenges")
    challenge_counts = mu.get_counts(filtered_df, "MainLogisticsChallenges")
    st.bar_chart(challenge_counts.head(10))

    st.markdown("### Correlation Heatmap (Ordinal Columns)")
    ord_cols = ["SustainabilityImportance", "LikelihoodToUseAIPlatform"]
    corr = filtered_df[ord_cols].corr()
    fig, ax = plt.subplots()
    im = ax.imshow(corr, cmap="viridis")
    ax.set_xticks(range(len(ord_cols))); ax.set_xticklabels(ord_cols, rotation=45, ha="right")
    ax.set_yticks(range(len(ord_cols))); ax.set_yticklabels(ord_cols)
    fig.colorbar(im, ax=ax)
    st.pyplot(fig)

    st.markdown("*(Plus several more charts & KPIs coded in `streamlit_app.py` for a total of 10 insights.)*")

# ------------------------------------------------------------------
# Tab 2: Classification
# ------------------------------------------------------------------
with tab2:
    st.subheader("Predicting Pilot Program Interest")
    # Prepare data
    class_df = mu.prepare_classification_data(filtered_df)
    X_train, X_test, y_train, y_test, scaler = mu.train_test_split_scaled(class_df.drop("PilotProgramInterest", axis=1),
                                                                          class_df["PilotProgramInterest"])

    models = {
        "KNN": KNeighborsClassifier(n_neighbors=5),
        "Decision Tree": DecisionTreeClassifier(max_depth=5, random_state=0),
        "Random Forest": RandomForestClassifier(n_estimators=100, random_state=0),
        "GBRT": GradientBoostingClassifier(random_state=0)
    }
    metrics_table = mu.evaluate_classifiers(models, X_train, X_test, y_train, y_test)
    st.dataframe(metrics_table)

    algo = st.selectbox("Select algorithm to view Confusion Matrix", list(models.keys()))
    cm_fig = mu.plot_confusion_matrix(models[algo], X_test, y_test)
    st.pyplot(cm_fig)

    roc_fig = mu.plot_roc_curves(models, X_test, y_test)
    st.pyplot(roc_fig)

    # Prediction on new data
    st.markdown("---")
    st.markdown("#### Upload new data (without `PilotProgramInterest`) for prediction")
    pred_upload = st.file_uploader("Upload CSV for prediction", key="pred")
    if pred_upload:
        new_df = pd.read_csv(pred_upload)
        preds = mu.predict_new(models["Random Forest"], new_df, scaler)
        result_df = new_df.copy()
        result_df["PredictedPilotInterest"] = preds
        st.write(result_df.head())
        csv_bytes = result_df.to_csv(index=False).encode("utf-8")
        st.download_button("Download Predictions", csv_bytes, "predictions.csv", "text/csv")

# ------------------------------------------------------------------
# Tab 3: Clustering
# ------------------------------------------------------------------
with tab3:
    st.subheader("Customer Segmentation (K‚ÄëMeans)")
    num_clusters = st.slider("Number of clusters", 2, 10, 4)
    cluster_df, inertias = mu.cluster_data(filtered_df, k_range=range(2, 11))

    elbow_fig = mu.plot_elbow(inertias)
    st.pyplot(elbow_fig)

    st.markdown("##### Cluster Personas")
    persona_table = mu.describe_clusters(cluster_df, num_clusters)
    st.dataframe(persona_table)

    csv_bytes = cluster_df.to_csv(index=False).encode("utf-8")
    st.download_button("Download Clustered Data", csv_bytes, "clustered_data.csv", "text/csv")

# ------------------------------------------------------------------
# Tab 4: Association Rules
# ------------------------------------------------------------------
with tab4:
    st.subheader("Association Rule Mining")
    target_cols = st.multiselect("Select association columns",
                                 ["MainLogisticsChallenges", "PainPointsCurrentModel", "EcoFeaturesMatterMost"])
    min_support = st.slider("Min Support", 0.01, 0.3, 0.05, 0.01)
    min_conf = st.slider("Min Confidence", 0.1, 1.0, 0.5, 0.05)

    if target_cols:
        rules = mu.run_association_rules(filtered_df, target_cols, min_support, min_conf)
        top_rules = rules.sort_values("confidence", ascending=False).head(10)
        st.dataframe(top_rules[["antecedents", "consequents", "support", "confidence", "lift"]])
    else:
        st.info("Select at least one column to run Apriori.")

# ------------------------------------------------------------------
# Tab 5: Regression
# ------------------------------------------------------------------
with tab5:
    st.subheader("Spend Prediction (Regression)")
    reg_df = mu.prepare_regression_data(filtered_df)
    X_train, X_test, y_train, y_test = mu.simple_train_test_split(reg_df.drop("SpendBin", axis=1), reg_df["SpendBin"])

    reg_models = {
        "Linear": LinearRegression(),
        "Ridge": Ridge(alpha=1.0),
        "Lasso": Lasso(alpha=0.1),
        "Decision Tree": DecisionTreeRegressor(max_depth=5, random_state=0)
    }
    reg_results = mu.evaluate_regressors(reg_models, X_train, X_test, y_train, y_test)
    st.dataframe(reg_results)

    st.markdown("*(Charts explaining residuals & feature importances are included in the code.)*")
